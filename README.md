Question answering is one of the most well-studied tasks in the field of NLP (Natural Language Processing), 
and a highlight of the last few years has been the emergence of BERT.

BERT and other Transformer-based models are now available for a variety of languages. 
However, for relatively minor languages, e.g. German, the available models are limited or do not exist at all. 
We faced several challenges when trying to fine-tune German BERT to a question answering task.

